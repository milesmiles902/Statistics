Statistics (Chapters 1-14)

Incomplete: 
>Chapter 4 - 38\
>Chapter 8 - 35, 41\
>Chapter 9 - 19d, 62\
>Chapter 10 - 10a, 23, 49

I began the tabulated datasets in Chapter 8. 

Cite: Rice, J. A. (2006). Mathematical Statistics and Data Analysis.. Belmont, CA: Duxbury Press.

Table of Contents:

    Preface
    1. Probability
    1.1. Introduction
    1.2. Sample Spaces
    1.3. Probability Measures
    1.4. Computing Probabilities: Counting Methods
    1.4.1. The Multiplication Principle
    1.4.2. Permutations and Combinations
    1.5. Conditional Probability
    1.6. Independence
    1.7. Concluding Remarks
    1.8. Problems
    2. Random Variables
    2.1. Discrete Random Variables
    2.1.1. Bernoulli Random Variables
    2.1.2. The Binomial Distribution
    2.1.3. The Geometric and Negative Binomial Distributions
    2.1.4. The Hypergeometric Distribution
    2.1.5. The Poisson Distribution
    2.2. Continuous Random Variables
    2.2.1. The Exponential Density
    2.2.2. The Gamma Density
    2.2.3. The Normal Distribution
    2.2.4. The Beta Density
    2.3. Functions of a Random Variable
    2.4. Concluding Remarks
    2.5. Problems
    3. Joint Distributions
    3.1. Introduction
    3.2. Discrete Random Variables
    3.3. Continuous Random Variables
    3.4. Independent Random Variables
    3.5. Conditional Distributions
    3.5.1. The Discrete Case
    3.5.2. The Continuous Case
    3.6. Functions of Jointly Distributed Random Variables
    3.6.1. Sums and Quotients
    3.6.2. The General Case
    3.7. Extrema and Order Statistics
    3.8. Problems
    4. Expected Values
    4.1. The Expected Value of a Random Variable
    4.1.1. Expectations of Functions of Random Variables
    4.1.2. Expectations of Linear Combinations of Random Variables
    4.2. Variance and Standard Deviation
    4.2.1. A Model for Measurement Error
    4.3. Covariance and Correlation
    4.4. Conditional Expectation and Prediction
    4.4.1. Definitions and Examples
    4.4.2. Prediction
    4.5. The Moment-Generating Function
    4.6. Approximate Methods
    4.7. Problems
    5. Limit Theorems
    5.1. Introduction
    5.2. The Law of Large Numbers
    5.3. Convergence in Distribution and the Central Limit Theorem
    5.4. Problems
    6. Distributions Derived from the Normal Distribution
    6.1. Introduction
    6.2. x[superscript 2], t, and F Distributions
    6.3. The Sample Mean and the Sample Variance
    6.4. Problems
    7. Survey Sampling
    7.1. Introduction
    7.2. Population Parameters
    7.3. Simple Random Sampling
    7.3.1. The Expectation and Variance of the Sample Mean
    7.3.2. Estimation of the Population Variance
    7.3.3. The Normal Approximation to the Sampling Distribution of X
    7.4. Estimation of a Ratio
    7.5. Stratified Random Sampling
    7.5.1. Introduction and Notation
    7.5.2. Properties of Stratified Estimates
    7.5.3. Methods of Allocation
    7.6. Concluding Remarks
    7.7. Problems
    8. Estimation of Parameters and Fitting of Probability Distributions
    8.1. Introduction
    8.2. Fitting the Poisson Distribution to Emissions of Alpha Particles
    8.3. Parameter Estimation
    8.4. The Method of Moments
    8.5. The Method of Maximum Likelihood
    8.5.1. Maximum Likelihood Estimates of Multinomial Cell Probabilities
    8.5.2. Large Sample Theory for Maximum Likelihood Estimates
    8.5.3. Confidence Intervals from Maximum Likelihood Estimates
    8.6. The Bayesian Approach to Parameter Estimation
    8.6.1. Further Remarks on Priors
    8.6.2. Large Sample Normal Approximation to the Posterior
    8.6.3. Computational Aspects
    8.7. Efficiency and the Cramer-Rao Lower Bound
    8.7.1. An Example: The Negative Binomial Distribution
    8.8. Sufficiency
    8.8.1. A Factorization Theorem
    8.8.2. The Rao-Blackwell Theorem
    8.9. Concluding Remarks
    8.10. Problems
    9. Testing Hypotheses and Assessing Goodness of Fit
    9.1. Introduction
    9.2. The Neyman-Pearson Paradigm
    9.2.1. Specification of the Significance Level and the Concept of a p-value
    9.2.2. The Null Hypothesis
    9.2.3. Uniformly Most Powerful Tests
    9.3. The Duality of Confidence Intervals and Hypothesis Tests
    9.4. Generalized Likelihood Ratio Tests
    9.5. Likelihood Ratio Tests for the Multinomial Distribution
    9.6. The Poisson Dispersion Test
    9.7. Hanging Rootograms
    9.8. Probability Plots
    9.9. Tests for Normality
    9.10. Concluding Remarks
    9.11. Problems
    10. Summarizing Data
    10.1. Introduction
    10.2. Methods Based on the Cumulative Distribution Function
    10.2.1. The Empirical Cumulative Distribution Function
    10.2.2. The Survival Function
    10.2.3. Quantile-Quantile Plots
    10.3. Histograms, Density Curves, and Stem-and-Leaf Plots
    10.4. Measures of Location
    10.4.1. The Arithmetic Mean
    10.4.2. The Median
    10.4.3. The Trimmed Mean
    10.4.4. M Estimates
    10.4.5. Comparison of Location Estimates
    10.4.6. Estimating Variability of Location Estimates by the Bootstrap
    10.5. Measures of Dispersion
    10.6. Boxplots
    10.7. Exploring Relationships with Scatterplots
    10.8. Concluding Remarks
    10.9. Problems
    11. Comparing Two Samples
    11.1. Introduction
    11.2. Comparing Two Independent Samples
    11.2.1. Methods Based on the Normal Distribution
    11.2.2. Power
    11.2.3. A Nonparametric Method-The Mann-Whitney Test
    11.2.4. Bayesian Approach
    11.3. Comparing Paired Samples
    11.3.1. Methods Based on the Normal Distribution
    11.3.2. A Nonparametric Method-The Signed Rank Test
    11.3.3. An Example-Measuring Mercury Levels in Fish
    11.4. Experimental Design
    11.4.1. Mammary Artery Ligation
    11.4.2. The Placebo Effect
    11.4.3. The Lanarkshire Milk Experiment
    11.4.4. The Portacaval Shunt
    11.4.5. FD&C Red No. 40
    11.4.6. Further Remarks on Randomization
    11.4.7. Observational Studies, Confounding, and Bias in Graduate Admissions
    11.4.8. Fishing Expeditions
    11.5. Concluding Remarks
    11.6. Problems
    12. The Analysis of Variance
    12.1. Introduction
    12.2. The One-Way Layout
    12.2.1. Normal Theory; the F Test
    12.2.2. The Problem of Multiple Comparisons
    12.2.3. A Nonparametric Method-The Kruskal-Wallis Test
    12.3. The Two-Way Layout
    12.3.1. Additive Parametrization
    12.3.2. Normal Theory for the Two-Way Layout
    12.3.3. Randomized Block Designs
    12.3.4. A Nonparametric Method-Friedman's Test
    12.4. Concluding Remarks
    12.5. Problems
    13. The Analysis of Categorical Data
    13.1. Introduction
    13.2. Fisher's Exact Test
    13.3. The Chi-Square Test of Homogeneity
    13.4. The Chi-Square Test of Independence
    13.5. Matched-Pairs Designs
    13.6. Odds Ratios
    13.7. Concluding Remarks
    13.8. Problems
    14. Linear Least Squares
    14.1. Introduction
    14.2. Simple Linear Regression
    14.2.1. Statistical Properties of the Estimated Slope and Intercept
    14.2.2. Assessing the Fit
    14.2.3. Correlation and Regression
    14.3. The Matrix Approach to Linear Least Squares
    14.4. Statistical Properties of Least Squares Estimates
    14.4.1. Vector-Valued Random Variables
    14.4.2. Mean and Covariance of Least Squares Estimates
    14.4.3. Estimation of [gamma superscript 2]
    14.4.4. Residuals and Standardized Residuals
    14.4.5. Inference about [beta]
    14.5. Multiple Linear Regression-An Example
    14.6. Conditional Inference, Unconditional Inference, and the Bootstrap
    14.7. Local Linear Smoothing
    14.8. Concluding Remarks
    14.9. Problems
    Appendix A. Common Distributions
    Appendix B. Tables
    Bibliography
    Answers to Selected Problems
    Author Index
    Applications Index
    Subject Index


